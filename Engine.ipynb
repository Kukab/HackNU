{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient()\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['articles']\n",
    "collection = db['aggr_arts_for_word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# f = open(\"sorted.json\", \"a\")\n",
    "# f.write(\"[\")\n",
    "# l = collection.find().distinct('_id')\n",
    "# for i in range(0,len(l)):\n",
    "#     e = collection.find_one({'_id': l[i]})\n",
    "#     list_of_art = e['articles']\n",
    "#     newlist = sorted(list_of_art, key=lambda k: k['word_count'], reverse=True) \n",
    "#     y = json.dumps({'_id':l[i], 'articles':newlist})\n",
    "#     f.write(y)\n",
    "#     f.write(\",\")\n",
    "#     if i%1000==0:\n",
    "#         print(i)\n",
    "        \n",
    "# f.write(\"]\")\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 23662\n",
      "2 166182\n",
      "3 140384\n",
      "4 147026\n",
      "5 151909\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = \"Copies of William Shakespeareâ€™s first four\"\n",
    "pre_t = text.lower()\n",
    "pre_t = re.sub('[^A-Za-z0-9]+', ' ', pre_t)\n",
    "\n",
    "word_tokens = word_tokenize(pre_t)  \n",
    "word_tags = nltk.pos_tag(word_tokens)\n",
    "word_tags_size = len(word_tags)\n",
    "\n",
    "#print(k, word_tags_size,\"\\n\\n\")\n",
    "\n",
    "#i - tokenized word index with POS\n",
    "pos = []\n",
    "no_sw_list = []\n",
    "\n",
    "for i in range(0, word_tags_size):\n",
    "    if word_tags[i][0] not in stop_words:\n",
    "        word = lemmatizer.lemmatize(word_tags[i][0], get_pos(word_tags[i][1]))\n",
    "        no_sw_list.append(word)\n",
    "\n",
    "no_sw_list\n",
    "\n",
    "collection_sort = db['sorted_articles_for_word']\n",
    "collection_map = db['word_article_map']\n",
    "\n",
    "set_pr = {}\n",
    "set_all = {}\n",
    "dict_article_fw_c = {}\n",
    "\n",
    "\n",
    "for i in range(0,len(no_sw_list)):\n",
    "    x = collection_sort.find_one({'_id': no_sw_list[i]})\n",
    "    #pprint.pprint(x)\n",
    "    list_of_art = x['articles']\n",
    "    #print(list_of_art)\n",
    "    #{d['article_id']:d['word_count'] for d in list_of_art}\n",
    "    dict_article_fw_c[no_sw_list[i]] = {d['article_id']:d['word_count'] for d in list_of_art}\n",
    "    \n",
    "#     sett_cur = {d['article_id'] for d in list_of_art}\n",
    "#     if set_pr:\n",
    "#         set_all = set_all\n",
    "    \n",
    "    \n",
    "#    collection_map.find_one({'word': no_sw_list[i], 'article_id'})\n",
    "\n",
    "pr_set = {}\n",
    "\n",
    "\n",
    "sets = dict_article_fw_c[no_sw_list[0]].keys()\n",
    "\n",
    "d_res = {}\n",
    "list_of_res = []\n",
    "\n",
    "if len(no_sw_list)==1:\n",
    "    d_res = dict_article_fw_c[no_sw_list[0]]\n",
    "    list_of_res = list(d_res.keys())[0:5]\n",
    "    \n",
    "else:\n",
    "    for i in range(0,len(no_sw_list)):\n",
    "        pr_set = sets\n",
    "        sets = sets & dict_article_fw_c[no_sw_list[i]].keys()\n",
    "        if not sets:\n",
    "            sets = pr_set\n",
    "\n",
    "    ls = list(sets)\n",
    "    ls = list(sets)\n",
    "    list_of_sum = []\n",
    "    new_dict = {}\n",
    "\n",
    "    for i in ls:\n",
    "        sum = 0\n",
    "        for j in dict_article_fw_c:\n",
    "            sum = sum+dict_article_fw_c[j][i]\n",
    "        new_dict[i] = sum\n",
    "    \n",
    "    list_of_res = list(dict(sorted(new_dict.items(), key=lambda item: item[1], reverse=True)).keys())\n",
    "\n",
    "for i in range(len(list_of_res)):\n",
    "    print(i+1,list_of_res[i])\n",
    "    if i==4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_map.find_one({'word': 'love', 'article_id':17283})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
