{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient()\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['articles']\n",
    "collection = db['aggr_arts_for_word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# f = open(\"sorted.json\", \"a\")\n",
    "# f.write(\"[\")\n",
    "# l = collection.find().distinct('_id')\n",
    "# for i in range(0,len(l)):\n",
    "#     e = collection.find_one({'_id': l[i]})\n",
    "#     list_of_art = e['articles']\n",
    "#     newlist = sorted(list_of_art, key=lambda k: k['word_count'], reverse=True) \n",
    "#     y = json.dumps({'_id':l[i], 'articles':newlist})\n",
    "#     f.write(y)\n",
    "#     f.write(\",\")\n",
    "#     if i%1000==0:\n",
    "#         print(i)\n",
    "        \n",
    "# f.write(\"]\")\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy\n",
    "import json\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "col_list = [\"id\", \"content\"]\n",
    "data = pd.read_csv(\"a123.csv\", usecols=col_list) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A central Texas man serving a life sentence for a double murder in 1992 is innocent, as are three codefendants no longer in prison, a state judge has found. Retired district judge George Allen ruled Friday that Richard Bryan Kussmaul, 45, should be free. His three codefendants each received   sentences and have already been released.  DNA evidence not available two decades ago shows the four weren’t involved in the fatal shootings of    Leslie Murphy and    Stephen Neighbors at a home near Moody, south of Waco, Allen said in a   opinion.  Allen’s recommendation now goes to the Texas court of criminal appeals for a final decision. The state’s top criminal appeals court had ordered a hearing held last month to look into the men’s claims they were innocent.  “The evidence just seemed overwhelming, with the DNA evidence having excluded these men from any contact whatsoever with these people,” Kussmaul’s lawyer, David Sheppard, told the Waco  . Allen presided over the trial where Kussmaul was convicted of murder. His three   James Edward Long, Michael Dewayne Shelton and James Wayne Pitts Jr, were convicted of sexual assault. Long and Pitts served 20 years. Shelton was released after 17 years.  At the hearing last month, Long, Shelton and Pitts all testified they gave false testimony against Kussmaul at his trial because a prosecutor promised them probation. They also said their confessions were coerced by a deputy who threatened them with the death penalty. The three said at Kussmaul’s trial in 1994 that he and they raped Murphy before Kussmaul shot the two victims. After they were sentenced to 20 years in prison, each recanted his confession. Kussmaul did not testify at the hearing.  “I was willing to say anything they wanted me to say because I thought I was getting probation and no prison time,” Long said at the July hearing. “I had two small children and I was afraid of going to prison for life or, worse, getting executed. ”  If the court upholds the judge’s opinion and the men’s claims of actual innocence, they each could be eligible for a state payout of $80, 000 for each year in prison. ']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  the election day\n",
      "\n",
      "\n",
      "Rank:  1\n",
      "Artcile ID:  56863\n",
      "\n",
      "Rank:  2\n",
      "Artcile ID:  206095\n",
      "\n",
      "Rank:  3\n",
      "Artcile ID:  78411\n",
      "\n",
      "Rank:  4\n",
      "Artcile ID:  78426\n",
      "\n",
      "Rank:  5\n",
      "Artcile ID:  36901\n",
      "\n",
      "----------------------- Time: 15.225021600723267 seconds -----------------------\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "\n",
    "collection_sort = db['sorted_articles_for_word']\n",
    "collection_map = db['word_article_map']\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = \"the election day\"\n",
    "pre_t = text.lower()\n",
    "pre_t = re.sub('[^A-Za-z0-9]+', ' ', pre_t)\n",
    "\n",
    "word_tokens = word_tokenize(pre_t)  \n",
    "word_tags = nltk.pos_tag(word_tokens)\n",
    "word_tags_size = len(word_tags)\n",
    "\n",
    "#print(k, word_tags_size,\"\\n\\n\")\n",
    "\n",
    "#i - tokenized word index with POS\n",
    "pos = []\n",
    "no_sw_list = []\n",
    "\n",
    "for i in range(0, word_tags_size):\n",
    "    if word_tags[i][0] not in stop_words:\n",
    "        word = lemmatizer.lemmatize(word_tags[i][0], get_pos(word_tags[i][1]))\n",
    "        if(collection_sort.find_one({'_id': word})):\n",
    "            no_sw_list.append(word)\n",
    "\n",
    "no_sw_list\n",
    "\n",
    "\n",
    "\n",
    "set_pr = {}\n",
    "set_all = {}\n",
    "dict_article_fw_c = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,len(no_sw_list)):\n",
    "    x = collection_sort.find_one({'_id': no_sw_list[i]})\n",
    "    list_of_art = x['articles']\n",
    "    dict_article_fw_c[no_sw_list[i]] = {d['article_id']:d['word_count'] for d in list_of_art}\n",
    "\n",
    "\n",
    "pr_set = {}\n",
    "\n",
    "\n",
    "sets = dict_article_fw_c[no_sw_list[0]].keys()\n",
    "\n",
    "d_res = {}\n",
    "d_check = {}\n",
    "list_of_res = []\n",
    "\n",
    "w_s = len(no_sw_list)\n",
    "\n",
    "# if ONE word\n",
    "if len(no_sw_list)==1:\n",
    "    d_res = dict_article_fw_c[no_sw_list[0]]\n",
    "    list_of_res = list(d_res.keys())[0:5]\n",
    "    \n",
    "else:\n",
    "    \n",
    "    #get articles that contains all of the keywords\n",
    "    for i in range(0,len(no_sw_list)):\n",
    "\n",
    "        sets = sets & dict_article_fw_c[no_sw_list[i]].keys()\n",
    "        if not sets:\n",
    "            break\n",
    "\n",
    "    ls = list(sets)\n",
    "    list_of_sum = []\n",
    "    new_dict = {}\n",
    "\n",
    "    for i in ls:\n",
    "        sum = 0\n",
    "        for j in dict_article_fw_c:\n",
    "            sum = sum+dict_article_fw_c[j][i]\n",
    "        new_dict[i] = sum\n",
    "    \n",
    "    list_of_res = list(dict(sorted(new_dict.items(), key=lambda item: item[1], reverse=True)).keys())\n",
    "    \n",
    "    \n",
    "    if list_of_res:\n",
    "        \n",
    "        #article next word count\n",
    "        dict_art_nw_c = {}\n",
    "        \n",
    "        for k in range(0,len(no_sw_list)-1):\n",
    "        \n",
    "            for i in range(len(list_of_res)):\n",
    "                \n",
    "                word = no_sw_list[k]\n",
    "                nword = no_sw_list[k+1]\n",
    "                \n",
    "                x = collection_map.find_one({'_id':{'word': word, 'article_id':list_of_res[i]}})\n",
    "#                 print(nword)\n",
    "#                 print(x[\"nw_key\"])\n",
    "#                 print(\"\\n\")\n",
    "                #print(list_of_res[i],word, nword, x[\"nw_key\"])\n",
    "                \n",
    "                if nword in x[\"nw_key\"]:\n",
    "        \n",
    "                    if list_of_res[i] in dict_art_nw_c:\n",
    "                        dict_art_nw_c[list_of_res[i]] += 1\n",
    "                        \n",
    "#                         if(dict_art_nw_c[list_of_res[i]]==(w_s-2)):\n",
    "#                             d_check[list_of_res[i]] = 1\n",
    "                        \n",
    "#                         if(d_check>5):\n",
    "#                             break\n",
    "                            \n",
    "                        #print(list_of_res[i])\n",
    "                    else:\n",
    "                        dict_art_nw_c[list_of_res[i]] = 1\n",
    "                        \n",
    "        list_of_res = list(dict(sorted(dict_art_nw_c.items(), key=lambda item: item[1], reverse=True)))    \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "#if list_of_res<5:\n",
    "    \n",
    "print(\"Sentence: \", text)\n",
    "print(\"\\n\")\n",
    "# for i in range(len(list_of_res)):\n",
    "#     print(\"Rank: \",i+1)\n",
    "#     print(\"Artcile ID: \",list_of_res[i])\n",
    "#     print(\"\\n\\n\")\n",
    "#     if i==4:\n",
    "#         break\n",
    "        \n",
    "        \n",
    "for i in range(len(list_of_res)):\n",
    "    print(\"Rank: \",i+1)\n",
    "    print(\"Artcile ID: \",list_of_res[i])\n",
    "    #print(\"Content: \", list(data['content'].loc[data['id'] == list_of_res[i]]))\n",
    "    print(\"\")\n",
    "    if i==4:\n",
    "        break\n",
    "        \n",
    "print(\"----------------------- Time: %s seconds -----------------------\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 151912\n"
     ]
    }
   ],
   "source": [
    "# collection_sort = db['sorted_articles_for_word']\n",
    "# collection_map = db['word_article_map']\n",
    "\n",
    "\n",
    "# stop_words = set(stopwords.words('english')) \n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# text = \"If the court upholds the judge’s opinion and the men’s claims of actual innocence, they each could be eligible for a state payout of $80, 000 for each year in prison. \"\n",
    "# pre_t = text.lower()\n",
    "# pre_t = re.sub('[^A-Za-z0-9]+', ' ', pre_t)\n",
    "\n",
    "# word_tokens = word_tokenize(pre_t)  \n",
    "# word_tags = nltk.pos_tag(word_tokens)\n",
    "# word_tags_size = len(word_tags)\n",
    "\n",
    "# #print(k, word_tags_size,\"\\n\\n\")\n",
    "\n",
    "# #i - tokenized word index with POS\n",
    "# pos = []\n",
    "# no_sw_list = []\n",
    "\n",
    "# for i in range(0, word_tags_size):\n",
    "#     if word_tags[i][0] not in stop_words:\n",
    "#         word = lemmatizer.lemmatize(word_tags[i][0], get_pos(word_tags[i][1]))\n",
    "#         no_sw_list.append(word)\n",
    "\n",
    "# no_sw_list\n",
    "\n",
    "# collection_sort = db['sorted_articles_for_word']\n",
    "# collection_map = db['word_article_map']\n",
    "\n",
    "# set_pr = {}\n",
    "# set_all = {}\n",
    "# dict_article_fw_c = {}\n",
    "\n",
    "\n",
    "# for i in range(0,len(no_sw_list)):\n",
    "#     x = collection_sort.find_one({'_id': no_sw_list[i]})\n",
    "#     #pprint.pprint(x)\n",
    "#     list_of_art = x['articles']\n",
    "#     #print(list_of_art)\n",
    "#     #{d['article_id']:d['word_count'] for d in list_of_art}\n",
    "#     dict_article_fw_c[no_sw_list[i]] = {d['article_id']:d['word_count'] for d in list_of_art}\n",
    "    \n",
    "# #     sett_cur = {d['article_id'] for d in list_of_art}\n",
    "# #     if set_pr:\n",
    "# #         set_all = set_all\n",
    "    \n",
    "    \n",
    "# #    collection_map.find_one({'word': no_sw_list[i], 'article_id'})\n",
    "\n",
    "# pr_set = {}\n",
    "\n",
    "\n",
    "# sets = dict_article_fw_c[no_sw_list[0]].keys()\n",
    "\n",
    "# d_res = {}\n",
    "# list_of_res = []\n",
    "\n",
    "# if len(no_sw_list)==1:\n",
    "#     d_res = dict_article_fw_c[no_sw_list[0]]\n",
    "#     list_of_res = list(d_res.keys())[0:5]\n",
    "    \n",
    "# else:\n",
    "#     for i in range(0,len(no_sw_list)):\n",
    "\n",
    "#         sets = sets & dict_article_fw_c[no_sw_list[i]].keys()\n",
    "#         if not sets:\n",
    "#             break\n",
    "\n",
    "        \n",
    "#     ls = list(sets)\n",
    "\n",
    "#     list_of_sum = []\n",
    "#     new_dict = {}\n",
    "\n",
    "#     for i in ls:\n",
    "#         sum = 0\n",
    "#         for j in dict_article_fw_c:\n",
    "#             sum = sum+dict_article_fw_c[j][i]\n",
    "#         new_dict[i] = sum\n",
    "    \n",
    "#     list_of_res = list(dict(sorted(new_dict.items(), key=lambda item: item[1], reverse=True)).keys())\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# for i in range(len(list_of_res)):\n",
    "#     print(i+1,list_of_res[i])\n",
    "#     if i==4:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_map.find_one({'word': \"go\", 'article_id':17283})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['canada', 'outside', 'shots', 'turkish', 'shia', 'accord', 'kill']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[\"nw_key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {'a':5, '3':5}\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
